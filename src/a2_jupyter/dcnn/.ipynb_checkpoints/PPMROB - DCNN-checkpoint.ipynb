{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "516a8a94",
   "metadata": {},
   "source": [
    "# PPMROB - Laborübung\n",
    "Dr. Dietmar Schreiner\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f917224f",
   "metadata": {},
   "source": [
    "# Deep Convolutional Neural Networks in pyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186dc2fb",
   "metadata": {},
   "source": [
    "## Zielsetzung\n",
    "\n",
    "In dieser Laborübung wird ein __Image Classification__ Network für den Fashion-MNIST (NIST = National Institute of Standards and Technology; M = modified) Datenbestand entwickelt, trainiert und evaluiert.\n",
    "\n",
    "### Fashion-MNIST\n",
    "\n",
    "Der Fashion-MNIST Datenbestand hat sich aus dem MNIST Datenbestand entwickelt, welcher eine Sammlung von 70.000 handgeschriebenen Ziffern ist. Der MNIST Benchmark ist allerdings überaltet und stellt für moderne Methoden keine interessante Herausforderung mehr dar.\n",
    "\n",
    "<img src=\"img/mnist_example.png\" alt=\"MNIST\" style=\"height: 200px;\"/> \n",
    "\n",
    "Im Lauf der Zeit wurden zahlreiche neue Aufgabenstellungen und Datenbestände entwickelt, publiziert und auch weltweit akzeptiert. Einer davon ist der sogenannte Fashion-MNIST Datenbestand. ([Zalando Research](https://github.com/zalandoresearch/fashion-mnist))\n",
    "\n",
    "Wie der ursprpngliche MNIST Datenbestand enthält auch der Fashion-MNIST Datenbestand 70.000 Bilder, die nun aber keine Ziffern sondern Produkte aus dem Sortiment von Zalando enthalten. Für den Benchmark werden die Bilder gleich dem MNIST Benchmark im Verhältnis 6:1 zwischen Training und Test aufgeteilt.\n",
    "\n",
    "<td> <img src=\"img/fmnist.png\" alt=\"Fashion-MNIST\" style=\"height: 200px;\"/> </td>\n",
    "\n",
    "#### Klassen (Labels)\n",
    "\n",
    "Der Fashion-MNIST Datenbestand ist in 10 Klassen eingeteilt.\n",
    "\n",
    "Index | Klasse/Label\n",
    ":---|:---\n",
    "0 | T-Shirt/Top\n",
    "1 | Hose\n",
    "2 | Pullover\n",
    "3 | Kleid\n",
    "4 | Mantel/Jacke\n",
    "5 | Sandale\n",
    "6 | Hemd\n",
    "7 | Turnschuh\n",
    "8 | Tasche/Beutel\n",
    "9 | Stiefelette\n",
    "\n",
    "#### Bildformat\n",
    "\n",
    "* Jedes Bild entspricht der Auflösung __28 x 28__ und umfasst 748 Pixel.\n",
    "* Jedes Pixel wird durch ein Byte repräsentiert, das den Dunkelheitswert des Bildpunkts speichert (je dunkler, je höher der Wert). \n",
    "\n",
    "#### Datenformat\n",
    "\n",
    "* Jede Zeile im Datenbestand repräsentiert ein Bild.\n",
    "* Spalte 0 enthält das Label.\n",
    "* Die Spalten 1 bis 748 enthalten das Bild (Dunkelheitswerte).\n",
    "\n",
    "\n",
    "## Vorgehensweise\n",
    "\n",
    "Im Bereich Machine Learning sieht die typische Vorgehensweise wie folgt aus:\n",
    "\n",
    "* Vorbereiten der Daten\n",
    "* Entwickeln des Modells\n",
    "* Trainieren des Modells\n",
    "* Analyse und Evaluation der Ergebnisse\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3104963",
   "metadata": {},
   "source": [
    "### Vorbereitung der Daten\n",
    "\n",
    "(Lern)Daten sind ein elementarer Baustein für die Leistungsfähigkeit eines Neuronalen Netzes. Die Vorbereitung bzw. die Aufbereitung der Daten spielt daher eine sehr wichtige Rolle. \n",
    "\n",
    "**Anmerkung: Datenherkunft**\n",
    "\n",
    "Da die zum Lernen benutzten Daten das Endergebnis des Lernprozesses definieren sind folgende Fragen von esentieller Wichtigkeit:\n",
    "\n",
    "- Wer hat den Datenbestand erzeugt?\n",
    "- Wie wurde der Datenbestand erzeugt?\n",
    "- Wurden die Daten einer Transformation unterzogen?\n",
    "- Mit welchem Ziel wurde der Datenbestand erstellt?\n",
    "- Ist der Datenbestand möglicherweise unausgewogen?\n",
    "\n",
    "#### ETL Prozess\n",
    "\n",
    "Der ETL Prozess zur Bereitstellung der Daten unterteilt sich in drei Schritte:\n",
    "\n",
    "* **E**xtract: Extrahieren der Daten aus einer Datenquelle\n",
    "* **T**ransform: Transformation der Daten in das gewünschte (eigene) Format\n",
    "* **L**oad: Laden der Daten in passende Datenstrukturen zur weiteren Analyse/Verarbeitung\n",
    "\n",
    "Für dieser Laborübung wird auf die vorhandenen Ressourcen des __torchvision__ Pakets zurückgegriffen. Diese sind:\n",
    "\n",
    "* __Datasets__ (z.B. MNIST, Fashion-MNIST)\n",
    "* __Models__ (z.B. vgg16)\n",
    "* Transforms\n",
    "* Utils\n",
    "\n",
    "__Aufgabe:__ Analyse des MNIST Packages, bzw. der Implementierung von Fashion-MNIST in ___torchvision___.\n",
    ">__Tip__: miniconda3 -> envs -> ppmrob -> Lib -> site-packages -> torchvision)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38be8325",
   "metadata": {},
   "source": [
    "##### Exctract & Transform\n",
    "\n",
    "Im ersten Schritt werden die Fashion-MNIST Bilddaten von der Datenquelle extrahiert. Im zweiten Schritt werden die Bilddaten in einen pyTorch Tensor übertragen. \n",
    "\n",
    "In pyTorch steht dazu die Klassen ___Dataset___ (abstrakte Klasse) zur Verfügung. Eine Kindklasse der abstrakten __Dataset__ Klasse muss zwingend eine Implementierung der Methoden __\\_\\_getitem()\\_\\___ und __\\_\\_len()\\_\\___ enthalten.\n",
    "\n",
    "```python\n",
    "class MyDataset(Dataset):\n",
    "    ...\n",
    "    def __getitem__(self, index):\n",
    "        ...\n",
    "        return sample, label\n",
    "    def __len__(self):\n",
    "        ...\n",
    "        return lenOfMyDataset\n",
    "    ...\n",
    "```\n",
    "\n",
    "Die vom ___torchvision___ Paket bereitgestellte Implementierung für den Fashion-MNIST Datenbestand erledigt Extraktion und Transformation innerhalb des Konstruktors der Fashion-MNIST Klasse.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a28110",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba00328f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for reference purpose\n",
    "print(torch.__version__)\n",
    "print(torchvision.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e5745a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = torchvision.datasets.FashionMNIST(\n",
    "    root = './data', # target directory for dataset\n",
    "    train = True, # select partition from which the data is taken from\n",
    "    download = True, # download if data does not exist\n",
    "    transform = transforms.Compose([transforms.ToTensor()]) # transform raw image data to tensow\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f08a92b",
   "metadata": {},
   "source": [
    "__Aufgabe:__ Analyse des Fashion-MNIST Datenbestandes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d8a7b3",
   "metadata": {},
   "source": [
    "##### Load\n",
    "\n",
    "Im dritten Schritt werden die Daten in ein Objekt verpackt, das den Datenzugriff und die Verwendung der Daten erleichtert. In pyTorch steht dazu die Klasse ___DataLoader___ zur Verfügung.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2654363",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_set, # dataset to operate on\n",
    "    batch_size=10 # batch size for the DL operation, default is 1\n",
    ")        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc845d1",
   "metadata": {},
   "source": [
    "##### Arbeiten mit den Daten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574e7824",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt # conda install -c conda-forge matplotlib\n",
    "\n",
    "torch.set_printoptions(linewidth=120)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe8cf32",
   "metadata": {},
   "source": [
    "###### Allgemeine Struktur des Datenbestands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33abdb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_set) # size of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34deb436",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set.targets # labels of all images within the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9a60f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set.targets.bincount() # frequency distribution of labels within tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5854e63d",
   "metadata": {},
   "source": [
    "Da alle Bins gleich viele Elemente anthalten wird dieser Datenbestand als ausgewogen (_balanced_) bezeichnet. Stehen die Ausgangsdaten nur unausgewogen (_unbalanced_) zur Verfügung ist es i.d.R. sinnvoll die Ausgewogenheit herzustellen, indem Elemente in unterrepräsentierten Klassen dupliziert werden (___Oversampling___)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4627813",
   "metadata": {},
   "source": [
    "###### Datenzugriff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c66df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = next(iter(train_set)) # get next single sample from stream of iterable objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c37364",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65d56e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b160a3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "image, label = sample # get image and lable via python's sequence unpacking mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71008019",
   "metadata": {},
   "outputs": [],
   "source": [
    "image.shape # examine shape of image tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ebc22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(image.squeeze(),cmap='gray')\n",
    "print('label:', label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269263f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(sample[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5f87e4",
   "metadata": {},
   "source": [
    "###### Batches\n",
    "\n",
    "Batches sind eine Zudammenfassung von mehreren (_n_) Bildern, also ein Tensor aus _n_ Bild-Tensoren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4dce554",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_loader)) # get batch from train_loader (not train_data!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570318e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913cee9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87112a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(batch[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06e5cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(batch[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c228e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95523636",
   "metadata": {},
   "outputs": [],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7005c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4bacdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = torchvision.utils.make_grid(images, nrow=10)\n",
    "plt.figure(figsize=(15,15)) # figure size 15x15\n",
    "plt.imshow(np.transpose(grid,(1,2,0)))\n",
    "\n",
    "print('labels:', labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805c6296",
   "metadata": {},
   "source": [
    "### Entwickeln des Modells\n",
    "\n",
    "Mit dem Begriff Modell (model) wird _das Neuronale Netzwerk selbst_, dessen Architektur, Parameter (learnable) und Hyperparameter, bezeichnet.\n",
    "\n",
    "Um ein Model zu entwickeln wird von PyTorch im Package ___torch.nn___ umfangreiche Funktionalität zur Verfügung gestellt, die dem OO-Paradigma folgt. Die Basisklasse für alle Neuronalen Netzwerke, wie auch für alle Module (Schichten mit trainierbaren Parametern) eines Neuronalen Netzes ist die Klasse ___Module___. Diese ist unter anderem zuständig für das Registrieren der trainierbaren Gewichte (Parameter) der Module.\n",
    "\n",
    "Die Implementierung eines Neuronalen Netzes in PyTorch efolgt in folgenden Schritten:\n",
    "\n",
    "* Erweitern der Basisklasse ___torch.nn.Module___\n",
    "* Hinzufügen der Layer in Form von Klassenattributen\n",
    "* Implementierung des Forward Passes des Neuronalen Netzes in der Methode ___forward()___\n",
    "\n",
    "#### Erweiterung der Basisklasse & Hinzufügen der Layer\n",
    "\n",
    "Um ein Neuronales Netz zu spezifizieren muss dessen Klasse von ___torch.nn.Module___ erben. Im  Konstruktor werden alle Layer des Neuronalen Netzes als Klassenattribute hinzugefügt. Die Layer selbst sind wieder Kinder der Basisklasse ___torch.nn.Module___.\n",
    "\n",
    "```python\n",
    "import torch.nn as nn\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    ...\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # define architecture here\n",
    "        # ...\n",
    "    ...\n",
    "```\n",
    "\n",
    "In PyTorch sind einige häufig benötigte Typen von Layer bereits vorimplementiert. Diese sind z.B.:\n",
    "\n",
    "* Linear Layers\n",
    "* Convolutional Layers\n",
    "* Reccurent Layers\n",
    "* Transformers\n",
    "* Data Manipulation Layers\n",
    "    * Max pooling Layers\n",
    "    * Normalization Layers\n",
    "    * Dropout Layers\n",
    "\n",
    "#### Forward Pass\n",
    "\n",
    "Der Datenfluss durch das Neuronale Netz wird in der Implementierung der Funktion ___forward()___ konkretisiert. Dabei wird mittels Fuktionskomposition der einzelnen Layer und Funktionale eine zusammengesetzte \"Gesamtfunktion\" des Moduls/NNs realsiert. \n",
    "PyTorch stellt im Paket ___torch.nn.functional___ zusätzlich für Neuronale Netze typische Funktionen, wie z.B. ___ReLU___ oder ___SoftMax___ zur Verfügung.\n",
    "\n",
    "```python\n",
    "import torch.nn as nn\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    ...\n",
    "    def forward(self, t):\n",
    "        # implement forward pass for the NN here\n",
    "        # the overall forward pass is simply a function concatenation\n",
    "        # ...\n",
    "        return t\n",
    "    ...   \n",
    "```\n",
    "\n",
    "Die Funktion ___forward()___ wird in PyTorch vom Funktor der Klasse (**\\_\\_call()\\_\\_**) aufgerufen, wenn eine Forward Propagation berechnet werden soll.\n",
    "\n",
    "```python\n",
    "# instantiate my neural network\n",
    "my_model = MyModel()\n",
    "\n",
    "# now feed in an input_tensor for the network's forward pass\n",
    "my_model(input_tensor)\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "#### Beispiel für ein einfaches Neuronales Netz\n",
    "\n",
    "Beispielhaft soll folgendes Neuronales Netz mit 100 Input-Neuronen implementiert werden, das ein Hidden-Layer mit 200 Neuronen und ein Output-Layer mit 10 Neuronen besitzt. Alle Layer sind _dense_.\n",
    "\n",
    "<img src=\"img/nn_example.png\" alt=\"MNIST\" style=\"height: 350px;\" align=\"left\"/> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7a9661",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c40bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.linear1 = nn.Linear(100, 200) # Hyper parameters, manually chosen\n",
    "        self.activation = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(200, 10) # Hyper parameters, manually chosen\n",
    "        self.softmax = nn.Softmax()\n",
    "        \n",
    "    def forward(self, t):\n",
    "        t = self.linear1(t)\n",
    "        t = self.activation(t)\n",
    "        t = self.linear2(t)\n",
    "        t = self.softmax(t)\n",
    "        return(t)\n",
    "    \n",
    "my_model = MyModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbea164",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show my_model\n",
    "my_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195424e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show specific layer of my_model\n",
    "my_model.linear2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdfc4fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show type of specific layer of my_model\n",
    "for base in my_model.linear1.__class__.__bases__:\n",
    "    print(base.__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f8476e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show my_model parameters\n",
    "for param in my_model.parameters():\n",
    "    print(param)\n",
    "    print(param.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dccbad9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show parameters of specific layer of my_model\n",
    "for param in my_model.linear1.parameters():\n",
    "    print(param)\n",
    "    print(param.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd699e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show learnable parameters of my_network\n",
    "for name, param in my_model.named_parameters():\n",
    "    print(name, '\\t\\t', param.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6d71e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show learnable parameters (weights) of specific layer\n",
    "print(my_model.linear1.weight)\n",
    "print(my_model.linear1.weight.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa5c06b",
   "metadata": {},
   "source": [
    "#### Funktionsweise eines Layers am Beispiel Linear Layer\n",
    "\n",
    "Um die Funktionsweise des PyTorch Linear Layers zu untersuchen wird eine lineare Transformation zuerst händisch implementiert:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9b4b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define input tensor\n",
    "in_features = torch.tensor([1,2,3,4], dtype=torch.float32)\n",
    "\n",
    "# define weight tensor\n",
    "weight_matrix = torch.tensor([\n",
    "    [1,2,3,4],\n",
    "    [2,3,4,5],\n",
    "    [3,4,5,6]\n",
    "], dtype=torch.float32)\n",
    "\n",
    "# multiply them\n",
    "weight_matrix.matmul(in_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0693e7a4",
   "metadata": {},
   "source": [
    "Nun wird die selbe Funktionalität in Form eines PyTorch Linear Layers realisiert:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4fc2386",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a linear layer of desired shape\n",
    "linear = nn.Linear(in_features=4, out_features=3)\n",
    "\n",
    "# apply input tensor \n",
    "linear(in_features) # this invoces forward(...) via __call__(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27cb6e7",
   "metadata": {},
   "source": [
    "Das Ergebins ist allerdings nicht das gewünschte, da PyTorch beim Erzeugen eines Layers die Gewichte standardmäßig mit Zufallswerten initialisiert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f2d5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(linear.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd6cf98",
   "metadata": {},
   "source": [
    "Um die oben händisch implementierte lineare Transformation richtigzustellen muss daher der\"zufällige\" Gewichtstensor mit der korrekten _weight_matrix_ ersetzt werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f24557",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set weight tensor to the desired one\n",
    "linear.weight = nn.Parameter(weight_matrix)\n",
    "\n",
    "# apply input tensor \n",
    "linear(in_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab944b3",
   "metadata": {},
   "source": [
    "Auch dieses Ergebnis ist nicht korrekt, auch wenn sich bereits eine gewisse Ähnlichkeit zum Soll-Wert zeigt. Der Grund hierfür ist der Bias, der in PyTorch für ein Linear Layer standardmäßig aktiviert ist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6b69bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# deactivate bias\n",
    "linear.bias=None # better provide argument False for parameter bias when constructing a linear layer\n",
    "\n",
    "# apply input tensor \n",
    "linear(in_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ef11bf",
   "metadata": {},
   "source": [
    "Zusammenfassend kann ein PyTorch Linear Layer formal beschrieben werden durch $y=Ax+b$ wobei\n",
    "\n",
    "| Variable | Bedeutung |\n",
    "|:---|:--- |\n",
    "| $A$ | Gewichte (Tensor) |\n",
    "| $x$ | Input Tensor |\n",
    "| $b$ | Bias Tensor |\n",
    "| $y$ | Output Tensor |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ebfc394",
   "metadata": {},
   "source": [
    "#### Convolutional Neuronal Network für Fashion-MNIST\n",
    "\n",
    "##### Modell\n",
    "\n",
    "Für die Klassifikation eines Bildes aus dem Fashion-MNIST Datenbestandes kann ein einfaches Convolutional Neural Network verwendet werden. Die Architektur (das Model) des Netzes entspricht nachfolgender Abbildung:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4601ee5",
   "metadata": {},
   "source": [
    "<img src=\"img/fmnist_network.jpg\" alt=\"MNIST\" style=\"height: 300px;\" align=\"left\"/> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570f7630",
   "metadata": {},
   "source": [
    "Das Modell besteht aus 2 **Convolution Layer**, beider mit quadratischen Filtern der Größe _5x5_ und **Max Pooling** der Form _2x2_ sowie 3 **Fully Connected Layer** (Dense Layer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6323b201",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5) # in: 1x image (28x28), out: 6 channels (12x12)\n",
    "        self.conv2 = nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5) # in: 6x feature map (12x12), out: 12 channels (4x4)\n",
    "        self.fc1 = nn.Linear(in_features=12*4*4, out_features=120) # in: 12x feature map (4x4) flattened, out: 120, dense\n",
    "        self.fc2 = nn.Linear(in_features=120, out_features=60) # in: 120 out: 60, dense \n",
    "        self.out = nn.Linear(in_features=60, out_features=10) # in: 60, out: 10 output classes\n",
    "        \n",
    "    def forward(self,t):\n",
    "        # (1) input layer (identity...)\n",
    "        t = t\n",
    "        \n",
    "        # (2) hidden convolution layer 1\n",
    "        t = self.conv1(t)\n",
    "        t = fn.relu(t)\n",
    "        t = fn.max_pool2d(t, kernel_size=2, stride=2)\n",
    "        \n",
    "        # (3) hidden convolution layer 2\n",
    "        t = self.conv2(t)\n",
    "        t = fn.relu(t)\n",
    "        t = fn.max_pool2d(t, kernel_size=2, stride=2)\n",
    "        \n",
    "        # (4) hidden linear layer 1\n",
    "        t = t.reshape(-1, 12*4*4)\n",
    "        t = self.fc1(t)\n",
    "        t = fn.relu(t)\n",
    "\n",
    "        # (5) hidden linear layer 2\n",
    "        t = self.fc2(t)\n",
    "        t = fn.relu(t)\n",
    "        \n",
    "        # (6) output layer\n",
    "        t = self.out(t)\n",
    "        \n",
    "        # this is not used in this example, as for learning we will use x-entropy (which does softmax by itself)\n",
    "        # t = fn.softmax(t, dim=1)\n",
    "\n",
    "        return(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c4a44e",
   "metadata": {},
   "source": [
    "Zur Berechnung der ___Output___ Hyperparameter kann für quadratische Bilder und Filter folgende Formel herangezogen werden:\n",
    "\n",
    "\\begin{align}\n",
    "O = \\dfrac{n-f+2p}{s}+1\n",
    "\\end{align}\n",
    "\n",
    "| Variable | Bedeutung |\n",
    "|:---|:---|\n",
    "| $n$ | Breite und Höhe des Bildes |\n",
    "| $f$ | Breite und Höhe des Filters |\n",
    "| $p$ | Padding |\n",
    "| $s$ | Stride |\n",
    "\n",
    "Allgemein (nicht-quadratisch) gilt:\n",
    "\n",
    "\\begin{align}\n",
    "O_h = \\dfrac{n_h-f_h+2p}{s}+1\n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    "O_w = \\dfrac{n_w-f_w+2p}{s}+1\n",
    "\\end{align}\n",
    "\n",
    "| Variable | Bedeutung |\n",
    "|:---|:---|\n",
    "| $n_w$ | Breite des Bildes |\n",
    "| $n_h$ | Höhe des Bildes |\n",
    "| $f_w$ | Breite des Filters |\n",
    "| $f_h$ | Höhe des Filters |\n",
    "| $p$ | Padding |\n",
    "| $s$ | Stride |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e02c3d0",
   "metadata": {},
   "source": [
    "### Inferenz (Forward Pass) \n",
    "\n",
    "\n",
    "Um am Ausgang dieses Netzwerks eine Aussage/Prognose zu einem bestimmten Bild zu erhalten, muß das Bild (eigentlich der Bildtensor) vorwärts durch das Netz propagiert werden. Dieser Vorgang wird als ___Inferenz___ bezeichnet.\n",
    "\n",
    "**Hinweis**: Zum Zeitpunkt der Inferenz ist das maschinelle Lernen des Neuronalen Netzes bereits abgeschlossen. Die in PyTorch dynamische Berechnung der Gradienten der Fehlerfunktion (benötigt für den Lernvorgang) des Netzwerks bei der Vorwärtspropagation eines Tensors wird daher nicht benötigt und sollte deaktiviert werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3f77ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# deactivate dynamic gradient calculation\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35696678",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the Fashion-MNIST network\n",
    "network = Network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123f759a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the model\n",
    "network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3593af33",
   "metadata": {},
   "source": [
    "#### Inferenz für ein einzelnes Bild"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d503361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a sample image\n",
    "sample = next(iter(train_set))\n",
    "image, label = sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97161d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "image.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1857ffb4",
   "metadata": {},
   "source": [
    "PyTorch erwartet als Input für Neuronale Netze nicht ein einzelnen Bild sondern einen Batch. \n",
    "> ___(batch_size, in_channels, height, width)___\n",
    "\n",
    "Dieser kann natürlich auch nur aus einem Bild bestehen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34758adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a batch containing exactly one image by add one dimension\n",
    "image.unsqueeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c757e3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run inference\n",
    "pred = network(image.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55255720",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9621b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred # outout tensor (raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ab807e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred.argmax(dim=1) # the index of the \"best\" (max) argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52088cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "fn.softmax(pred, dim=1) # probability for each class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2753a73f",
   "metadata": {},
   "source": [
    "Das Ergebnis ist natürlich unbefriedigend. Dies liegt daran, dass das verwendete Neuronale Netzwerk (noch) nicht trainiert ist."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9756b663",
   "metadata": {},
   "source": [
    "#### Inferenz für einen Batch\n",
    "\n",
    "Eines PyTorch Batche ist (wie bereits oben gezeigt) ein Tensor folgender ausprägung:\n",
    "> ___(batch_size, in_channels, height, width)___\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a06872e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get next batch from data loader\n",
    "batch = next(iter(train_loader))\n",
    "\n",
    "# decompose batch\n",
    "images, labels = batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3727b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9bd36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4cb535",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run inference\n",
    "preds = network(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f1dd5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output tensor (raw)\n",
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c8d0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb6e3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# most likely class for each image\n",
    "preds.argmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216f2788",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ground truth\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88eaf4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparison of result to ground truth\n",
    "preds.argmax(dim=1).eq(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85768416",
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_of_correct_preds(preds, labels):\n",
    "    return preds.argmax(dim=1).eq(labels).sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f18cb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_correct_preds(preds, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06dffb17",
   "metadata": {},
   "source": [
    "### Trainieren des Modells\n",
    "\n",
    "Der Vorgang des maschinellen Lernens in einem Neuronalen Netzes umfasst folgende Schritte:\n",
    "\n",
    "1. Batch von Trainingsdaten (aus der Menge der Trainingsdaten) vorbereiten\n",
    "2. Vorwärtspropagation des Batches durch das Neuronale Netz\n",
    "3. Berechnung des Fehlers (loss)\n",
    "4. Berechnung der Gradienten der Fehlerfunktion unter Berücksichtigung der Gewichte\n",
    "5. Aktualisieren der Gewichte unter Zuhilfenahme der Gradienten\n",
    "\n",
    "Diese Schritte werden für eine gesamte **Epoche** wiederholt. Unter eine Epoche versteht man die Gesamtmenge der Trainingsdaten.\n",
    "\n",
    "Solange die Güte (Genauigkeit) des Netzwerks nicht der gewünschten entspricht wiederhole den gesamten Vorgang für weitere Epochen. \n",
    "\n",
    "**Anmerkung**: Sollte ein Modell auch nach umfangreichem wiederholten Training die geforderte Güte klar nicht erreichen, kann es sinnvoll sein, den Trainingsvorgang abzubrechen und das Modell zu überarbeiten. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3786c204",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do some house keeping\n",
    "torch.set_grad_enabled(True) # should be on by default, but maybe has been turned off above"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a91fc34",
   "metadata": {},
   "source": [
    "#### Vorbereitung eines Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2625d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new train_loader with batch size of 100\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=100)\n",
    "\n",
    "# get a batch of training data\n",
    "batch = next(iter(train_loader))\n",
    "\n",
    "images, labels = batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1994b59",
   "metadata": {},
   "source": [
    "#### Vorwärtspropagation des Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a391136f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate predictions for the batch\n",
    "preds = network(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19340ecd",
   "metadata": {},
   "source": [
    "#### Beechnung des Fehlers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bcc616c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate loss for the batch\n",
    "loss = fn.cross_entropy(preds, labels)\n",
    "loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1fd19d6",
   "metadata": {},
   "source": [
    "Die Interpretation dieses Wertes hängt von der gewählten Fehlerfunktion ab. Mit voranschreitendem Training des Netzes sollte sich der Wert verringern."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df935cb8",
   "metadata": {},
   "source": [
    "#### Berechnung der Gradienten\n",
    "\n",
    "Vor dem ersten Aufruf der Back-Propagation Funktion existieren (noch) keine Gradienten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82ef445",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prior to the first call of backward() no gradients exist\n",
    "print(network.conv1.weight.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8c9f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate gradients\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a700cc",
   "metadata": {},
   "source": [
    "Nach Durchführung der Back-Propergation stehen berechnete Gradienten zur Verfügung. Für jeden Tensor mit Gewichten wird ein korrespondierender Tensor mit den Gradienten der Gewichte erzeugt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f16f722",
   "metadata": {},
   "outputs": [],
   "source": [
    "network.conv1.weight.grad.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6dca97e",
   "metadata": {},
   "source": [
    "#### Aktualisieren der Gewichte\n",
    "\n",
    "Um die Gewichte des Neuronalen Netzes neu einzustellen, was eben der Lernvorgang ist, werden diese mittels eines sogenannten ___Optimizers___ aktualisiert. Gängige Optimizer sind:\n",
    "\n",
    "* **Stochastic Gradient Decent (SGD)**: Die Gewichtsänderung entspricht dem Produkt aus Lernrate und Gradient.\n",
    "* **Root Mean Square Propagation (RMSProp)**: Die Gewichtsänderung hängt von einem exponentiell gleitenden Durchschnitt vorherigen Gradienten und einer daraus abgeleiteten Lernrate ab.\n",
    "* **Adaptive Moment Estimation (ADAM)**: Dieser Optimizer kombiniert RMSProp mit einem Momentum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76b3d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import optimizer package\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9074540",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(network.parameters(), lr=0.01) # lr is the hyper parameter \"learning rate\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f9a6c8",
   "metadata": {},
   "source": [
    "##### Training mit einem einzelnen Batch\n",
    "Vor dem Lernvorgang:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab9c149",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loss: \",loss.item())\n",
    "print(\"Correct items:\", num_of_correct_preds(preds, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28476a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# update the weights\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00fe43a6",
   "metadata": {},
   "source": [
    "Nach dem Lernvorgang:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b447c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get predictions for the same images\n",
    "preds = network(images)\n",
    "\n",
    "# calculate new loss\n",
    "loss = fn.cross_entropy(preds, labels)\n",
    "\n",
    "print(\"Loss: \",loss.item())\n",
    "print(\"Correct items:\", num_of_correct_preds(preds, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9edba2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do some house keeping\n",
    "enumerate(train_loader) # just reset the data loader for further usage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fcb8604",
   "metadata": {},
   "source": [
    "##### Training einer Epoche\n",
    "\n",
    "Um eine gesamte Epoche zu trainieren muss mit dem ___DataLoader___ über den gesamten datenbestand iteriert werden. Dabei wird jeder Batch wie oben gezeigt abgearbeitet.\n",
    "\n",
    "**Wichtig**: PyTorch akkumuliert alle Gradienten zwischen aufeinanderfolgenden Aufrufen der ___backward()__ Funktion. Da das für den Lernvorgang nicht gewünscht wird, müssen vor dem Berechnen der Gradienten des Batches alle Werte des vorausgegangenen Batches mittels der Funktion ___zero.grad()___ auf 0 zurückgesetzt werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c04100",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_loss = 0\n",
    "total_correct = 0\n",
    "\n",
    "def do_epoche(epoche_loss, epoche_correct):\n",
    "    # iterate over all batches\n",
    "    for batch in train_loader:\n",
    "    \n",
    "        images, labels = batch # prepare batch\n",
    "    \n",
    "        preds = network(images) # forward pass batch\n",
    "        loss = fn.cross_entropy(preds, labels) # calculate loss\n",
    "    \n",
    "        optimizer.zero_grad() # reset gradient tensors (would be accumulated otherwise)\n",
    "    \n",
    "        loss.backward() # calculate gradients\n",
    "        optimizer.step() # update weights\n",
    "    \n",
    "        epoche_loss+=loss.item()\n",
    "        epoche_correct+=num_of_correct_preds(preds, labels)\n",
    "    \n",
    "    return epoche_loss, epoche_correct\n",
    "\n",
    "total_loss, total_correct = do_epoche(total_loss, total_correct)\n",
    "\n",
    "print(\"epoche:\", 0, \"total correct:\", total_correct, \"loss:\", total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1790d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_correct / len(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52fa10e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do some house keeping\n",
    "enumerate(train_loader) # reset the data loader for further usage\n",
    "network = Network() # create a new untrained network\n",
    "optimizer = optim.Adam(network.parameters(), lr=0.01) # create a new optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29609fab",
   "metadata": {},
   "source": [
    "##### Training über mehrere Epochen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6209e31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "for epoche in range(5):\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    \n",
    "    total_loss, total_correct = do_epoche(total_loss, total_correct)\n",
    "    print(\"epoche:\", epoche, \"total correct:\", total_correct, \"loss:\", total_loss, \"accuracy:\", total_correct / len(train_set))\n",
    "\n",
    "print(\"time:\", time.time() - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a28e02",
   "metadata": {},
   "source": [
    "#### Training auf der GPU\n",
    "\n",
    "PyTorch ermöglicht eine komfortable Nutzung von GPU Ressourcen bei der Verwendung Neuronaler Netze. Source Code kann mit geringen anpassungen sogar Hardware-Agnostisch entwickelt werden.\n",
    "\n",
    "Bei der Nutzung einer GPU (___CUDA___) ist darauf zu achten, dass alle Operanden (das Neuronale Netz, sowie alle Input Tensoren wie z.B. Batch der Bilder und Labels) auf der GPU instanziert werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c89cc6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use GPU if available, else use the CPU for calculations\n",
    "if torch.cuda.is_available():\n",
    "    target_device=\"cuda\"\n",
    "else:\n",
    "    target_device=\"cpu\"\n",
    "    \n",
    "device=torch.device(target_device)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68461762",
   "metadata": {},
   "outputs": [],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d494b918",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new neural network and transfer it to the GPU memory\n",
    "network = Network().to(device) # move NN to GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc46f464",
   "metadata": {},
   "outputs": [],
   "source": [
    "for n, p in network.named_parameters():\n",
    "    print(p.device, '', n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d15ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do some house keeping\n",
    "enumerate(train_loader) # just reset the data loader for further usage\n",
    "\n",
    "# create a new optimizer that is based on the GPU memory of the new network\n",
    "optimizer = optim.Adam(network.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eeec48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "for epoche in range(5):\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    \n",
    "    # iterate over all batches\n",
    "    for batch in train_loader:\n",
    "        images = batch[0].to(device)\n",
    "        labels = batch[1].to(device) \n",
    "    \n",
    "        preds = network(images) # forward pass batch\n",
    "        loss = fn.cross_entropy(preds, labels) # calculate loss\n",
    "    \n",
    "        optimizer.zero_grad() # reset gradient tensors (would be accumulated otherwise)\n",
    "    \n",
    "        loss.backward() # calculate gradients\n",
    "        optimizer.step() # update weights\n",
    "    \n",
    "        total_loss+=loss.item()\n",
    "        total_correct+=num_of_correct_preds(preds, labels)\n",
    "\n",
    "    print(\"epoche:\", epoche, \"total correct:\", total_correct, \"loss:\", total_loss, \"accuracy:\", total_correct / len(train_set))\n",
    "\n",
    "print(\"time:\", time.time() - start_time)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af262b78",
   "metadata": {},
   "source": [
    "### Analyse und Evaluation der Ergebnisse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b57c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do some house keeping\n",
    "enumerate(train_loader) # just reset the data loader for further usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2eefeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reinstantiate Network\n",
    "network = Network()\n",
    "\n",
    "# create a new optimizer that is based on the memory of the new network\n",
    "optimizer = optim.Adam(network.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f106883d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the Network\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "for epoche in range(5):\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    \n",
    "    total_loss, total_correct = do_epoche(total_loss, total_correct)\n",
    "    print(\"epoche:\", epoche, \"total correct:\", total_correct, \"loss:\", total_loss, \"accuracy:\", total_correct / len(train_set))\n",
    "\n",
    "print(\"time:\", time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcdaa117",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function that feeds all datasets from a dataloader into the neural net and\n",
    "# returns a tensor with all predictions for the input data set\n",
    "def get_all_preds(model, loader):\n",
    "    all_preds = torch.tensor([])\n",
    "    for batch in loader:\n",
    "        images, labels = batch\n",
    "        preds = model(images)\n",
    "        all_preds = torch.cat((all_preds, preds), dim=0)\n",
    "    return all_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e312dfd",
   "metadata": {},
   "source": [
    "#### Evaluation mit Testdaten\n",
    "\n",
    "Im Zuge der Datenaufbereitung wurden alle Verfügbaren Daten in Trainings- und Testdaten partitioniert. Der Trainingsprozess ist mit den Trainingsdaten vollzogen worden, für die Evaluierung des trainierten Neuronalen Netzes kommen die Testdaten, die das Neuronale Netz noch nie gesehen hat, zum Einsatz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ddf83f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = torchvision.datasets.FashionMNIST(\n",
    "    root = './data', # target directory for dataset\n",
    "    train = False, # select partition from which the data is taken from\n",
    "    download = True, # download if data does not exist\n",
    "    transform = transforms.Compose([transforms.ToTensor()]) # transform raw image data to tensow\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359cea43",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd11682f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_set, # dataset to operate on\n",
    "    batch_size=10000 # batch size for the DL operation, default is 1\n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372aed4c",
   "metadata": {},
   "source": [
    "Da beim Inferieren des Neuronalen Netzes kein Lernvorgang stattfindet, können alle Funktionen der Berechnungsverfolgung für Gradienten deaktiviert werden. Dies kann auch auf lokaler Ebene z.B. mittels eines Context Managers erfolgen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da0a285",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate predictions\n",
    "with torch.no_grad(): # context manager deactivates gradient calculation\n",
    "    test_preds = get_all_preds(network, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1f9065",
   "metadata": {},
   "source": [
    "Eine weitere Möglichkeit die dynamische Berechnung der Gradienten lokal zu deaktivieren ist die Annotation einer Funktion mittels eines Dekorators:\n",
    "\n",
    "```python\n",
    "@torch.no_grad()\n",
    "def get_all_preds(model, loader):\n",
    "    ...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199e9dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "total = len(test_set.targets) # total number of images processed\n",
    "total_correct = num_of_correct_preds(test_preds, test_set.targets) # number of correct predicitions\n",
    "\n",
    "print(\"Network accuracy: \", total_correct/total, \" got \", total_correct, \" right from \", total )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b25e25",
   "metadata": {},
   "source": [
    "#### Confusion Matrix\n",
    "\n",
    "Eine sogenannte ___Confusion Matrix___ ist eine 3-dimensionale Matrix die darüber Auskunft gibt, welche Klassen korrekt erkannt wurden, und bei welchen Klassen Abweichungen in der Vorhersage vorliegen.\n",
    "\n",
    "Dazu werden auf den zwei orthogonalen Achsen der Grundebene (x,y) jeweils alle Labels (Klassen) aufgetragen, um die Vorhersage des Neuronalen Netzes mit dem tatsächlichen Wert in Relation zu setzen. Die erste Achse (x) repräsentiert die Vorhersagen des Netzwerks und die zweite Achse (y) die tatsächlichen Werte (_ground truth_). In der dritten Dimension (z) wird die Gesamtzahl aller zur jewiligen Paarung aus Vorhersage und tatsächlichem Wert gehörenden Ausgaben (x,y) des Neuronalen Netzes gezählt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04977e71",
   "metadata": {},
   "source": [
    "<img src=\"img/cm.jpg\" alt=\"Confusion Matrix\" style=\"height: 120px;\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adbb1011",
   "metadata": {},
   "source": [
    "In einem perfekt trainierten Netzwerk (_accuracy=100%_) wird nur die erste Hauptdiagonale der Grundebene Werte $\\gt 0$ für die z-Achse enthalten, da alle Klassenprognosen exakt mit den tatsächlichen Klassen übereinstimmen. In _echten_ Neuronalen Netzen werden auch abseits der Hauptdiagonale Werte auftreten (eben falsche Prognosen). Mittels der Confusion Matrix wird dadurch deutlich bei welchen Klassen Prognosen scheitern, und vor allem was womit verwechselt wird."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f7da51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data loader based on training data (!)\n",
    "pred_loader = torch.utils.data.DataLoader(train_set, batch_size=10000)\n",
    "\n",
    "# calculate predictions\n",
    "with torch.no_grad(): # context manager deactivates gradient calculation\n",
    "    train_preds = get_all_preds(network, pred_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54585f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224aafef",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preds.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8c444a",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_correct_preds(train_preds, train_set.targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f90d1f",
   "metadata": {},
   "source": [
    "##### Berechnung der Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb11564",
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels (ground truth)\n",
    "train_set.targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a266c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions\n",
    "train_preds.argmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c459fad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# map ground truth to prediction for each prediction\n",
    "stacked = torch.stack(\n",
    "    (\n",
    "        train_set.targets, \n",
    "        train_preds.argmax(dim=1)\n",
    "    ),\n",
    "    dim=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea840ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ff9ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a1004b",
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94fa1043",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmt = torch.zeros(10, 10, dtype=torch.int64)\n",
    "cmt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9705aaed",
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in stacked:\n",
    "    gt, pred = p.tolist()\n",
    "    cmt[gt, pred] = cmt[gt, pred] + 1\n",
    "    \n",
    "cmt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103b4405",
   "metadata": {},
   "source": [
    "##### Plot der Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079df9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix # scikit-learn\n",
    "from resources.plotcm import plot_confusion_matrix # local include\n",
    "\n",
    "cm = confusion_matrix(train_set.targets, train_preds.argmax(dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5023c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e9185f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32817f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = (\"T-Shirt/Top\", \"Hose\", \"Pullover\", \"Kleid\", \"Mantel/Jacke\", \"Sandale\", \"Hemd\", \"Turnschuh\", \"Tasche/Beutel\", \"Stiefelette\")\n",
    "plt.figure(figsize=(10,10))\n",
    "plot_confusion_matrix(cm, names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd947e2",
   "metadata": {},
   "source": [
    "### Speichern und Laden eines trainierten Neuronalen Netzes\n",
    "\n",
    "Das erlernte \"Wissen\" eines Neuronalen Netzes befindet sich in den erlernbaren Parametern. Um dieses zu erhalten, und somit das trainierte Neuronale Netz zu persisitieren, können diese Parameter gespeichert und geladen werden. Ein erfolgreich trainiertes Neuronales Netz kann dadurch mittels Modelldefinition und gespeicherter Parameter wiederhergestellt werden, ohne einen neuerlichen Lernprozess erforderlich zu machen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4ad243",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"nets/fashionMNIST.pt\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10e5e25",
   "metadata": {},
   "source": [
    "#### Speichern\n",
    "\n",
    "Die Funktion ___torch.save()___ speichert ein komplettes Modell in eine Datei (zip) unter der Verwendung von Pickle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c82b4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(\n",
    "    network, # neural network to be saved\n",
    "    file_name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c4a44e",
   "metadata": {},
   "source": [
    "#### Laden\n",
    "Die Funktion ___torch.load()___ läd ein komplettes mittels ___torch.save()___ gespeichertes Neuronales Netz.\n",
    "\n",
    "**Anmerkung**: Nach dem Aufruf von ___torch.load()___ sollte die Funktion ___eval()___ des Models aufgerufen werden um ___dropout___ und ___batch normalization___ ___Layer___ für den Inferenzvorgang korrekt zu initialisieren. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9baeab1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "network2 = torch.load(\n",
    "    file_name,\n",
    "    weights_only=False\n",
    ")\n",
    "network2.eval()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PPMROB",
   "language": "python",
   "name": "ppmrob"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
